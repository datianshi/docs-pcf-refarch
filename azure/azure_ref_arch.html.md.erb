---
title: Azure Reference Architecture
owner: Customer0
---

This guide presents a reference architecture for Pivotal Cloud Foundry (PCF) on Azure.


## <a id="overview"></a> Overview

Azure Virtual Data Center (VDC) uses a hub and spoke model for extending on-premises data centers.

When installing PCF, you can use this model to place resources in resource groups. For example, a PCF installation contains the following:

* **Hub**: 
	* A central IT resource group included in Azure VDC setup. This is where you define firewall, access control, and security policies. 
* **Spokes**: 
  * A Network resource group for all network resource
  * PAS/PKS resource groups for all PAS/PKS resources

See the following architecture diagram, which illustrates this model: 

![alt_text](../images/PAS_Azure_with_AZs.png) 

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/PAS_Azure_with_AZs.png).

 For more information about VDC, see the [Azure Virtual Datacenter e-book](https://azure.microsoft.com/en-us/resources/azure-virtual-datacenter/).

You do not need to make any configuration changes in PCF to support Azure DNS.

## <a id="networking"></a> Networking

### <a id="network-considerations"></a> General Considerations

* You must enable Virtual Network peering between hub and spoke resource groups. See [Virtual network peering](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-peering-overview). 
* You can use Express routes or VPN Gateway to enable connectivity from an on-premises data center to Azure Hub resource groups. See [ExpressRoute Documentation](https://docs.microsoft.com/en-us/azure/expressroute/).  
* You can place network resources, such as DNS and NTP servers, either on-premises or on Azure cloud. 
* A separate subscription for central network resource group (Hub) and each workspace where each workload runs (spoke) is needed.
*   If you want Internet ingress, create the load balancers with elastic IPs
*   Use ExpressRoutes for dedicated connection from on-premises datacenter to VDC
*   Use Azure VPN Gateways for passing encrypted data into VDC via internet.  Use this option as backup, if Express Routes goes down.
*   Setup a central firewall in the Hub resource group to enhance and to do data infiltration - data coming in and also data exfiltration using network virtual appliance.

### <a id="lb-types"></a>  Load Balancing

Use Standard Azure Load Balancers (ALBs). 

Consider the following:

* The ALBs must be configured for PAS Gorouters. 
* The PAS system and applications domains should resolve to this ALB and have either a private or a public IP address assigned to it. 
* When applicable, TCP Routers and SSH Proxies also require load balancers.

### <a id="networks"></a> Networks

PCF requires the networks defined in the base reference architectures for PAS and PKS. See the _Networks_ sections in [Platform Architecture and Planning](./index.html). 

### <a id="dns"></a> DNS

You can use Azure DNS zones for PCF domains. Azure DNS supports DNS delegation, which allows sub-level domains to be hosted within Azure. Pivotal recommends that you use a sub-zone for your PCF deployment. For example, if your company’s domain is `example.com`, your PCF zone in Azure DNS would be `pcf.example.com`.

Azure DNS does not support recursion. To properly configure Azure DNS, do the following:

1. Create a `NS `record with your registrar that points to the four name servers supplied by your Azure DNS Zone configuration. 
1. Create the required wildcard `A` records for the PCF app and system domains, as well as any other records desired for your PCF deployments.

### <a id="pas-single-azure-resource"></a> PAS on a Single Azure Resource Group

If shared network resources do not exist in an Azure subscription, you can use a single resource group to deploy PCF and define network constructs.

## <a id="ha-considerations"></a>  High Availability

As of PCF v2.5.3, PCF on Azure supports both Availability Zones and Availability Sets. Pivotal recommends that PCF deployments use Availability Zones over Availability Sets when possible. 

For more information about availability modes in Azure, see [Manage the availability of Windows virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-manage-availability) in the Microsoft Azure documentation. 

### <a id="region-internal-ha"></a> Region-Internal High Availability

Availability Sets were the first Azure primitive to support High Availability (HA) scenarios within a region. They are a logical concept that ensures multiple VMs in the same set are spread evenly across 3 Fault Domains and either 5 or 20 Update Domains. VMs on independent Fault Domains are guaranteed not to share power supply or networking and will therefore maintain availability of a subset of VMs in the set when a hardware issue occurs. VMs in independant Update Domains, when updates are rolled out to a Cluster, will not be updated at the same time, ensuring downtime incurred by an update does not affect the availability of the set. 

Availability Sets are implemented at the Cluster level in Azure, Clusters being a grouping of hardware for Azure Compute, this means Availability Sets cannot span VMs backed by different Clusters; for this reason, as Bosh places VMs for Jobs within the same Availability Sets, VMs for a particular Job are pinned to the Cluster where the first VM spins up. The Cluster Pinning problem leads to issues migrating Jobs from one Family of VMs to another, backed by different hardware, such as `DS_v2` and `DS_v3`, as well as issues scaling up if there is no capacity on the given cluster.



![alt_text](../images/PAS_Azure.png) 

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/PAS_Azure.png).

Availability Zones are a newer Azure primitive supporting HA scenarios within a region. AZs are physically separate data centers in the same region, each comprised of separate power, cooling and network. VMs and other Zonal services deployed to independent Zones provide greater physical separation than Fault Domains and avoid the Cluster Pinning problem described above. Zonal services are also effectively in separate Update Domains as updates will not be rolled out to another Zone before the other is complete. Finally, Availability Zones are visible and configurable to the customer. This means two Zonal services placed in the same logical Zone are placed in the same physical Zone, allowing services of different types to take advantage of the HA qualities a Zone provides.


### <a id="migration"></a> Migration to Availability Zones

Availability Zones should be used as the HA primitive for PCF in all Azure Regions where they are available. Availability Zones solve the Cluster Pinning issue and allow customers greater control when defining the HA configuration when using Azure multiple services. 

New Foundations deployed to AZ enabled Regions should use AZs over Availability Sets. New Foundations without hard requirements should consider selecting AZ enabled Regions; some hard requirements include existing ExpressRoute connections from non-AZ enabled Regions to customer data centers and application specific latency requirements. 

Existing Foundations in AZ enabled Regions should plan to migrate to a new PCF v2.5.3+ Foundation; these existing Foundations, especially those using older Families of VMs like `DS_v2`, risk running out of capacity as they scale. For existing Foundations in non-AZ enabled Regions the customer requirements should be evaluated to determine the cost/benefit of migration to an AZ enabled Region.


## <a id="storage-considerations"></a> Storage Considerations

All Azure Storage Accounts should be Premium Skew and should be configured for Zone Redundant Storage (ZRS) in Availability Zones enabled Regions. In Non-AZ enabled Regions Locally Redundant Storage (LRS) is sufficient. 

All VMs should be using Managed Disks instead manually managed VHDs on Storage Accounts. Managed Disks are an Azure feature that handles the correct distribution of VHDs over Storage Accounts to maintain high IOPS as well as HA for the VMs.

PCF on Azure requires 5 standard storage accounts: BOSH, Ops Manager, and three PAS storage accounts. Each account comes with a set amount of disk. Reference architecture recommends using 5 storage accounts because Azure Storage Accounts have an IOPs limit of approximately 20k per account, which generally relates to a BOSH JOB/VM limit of approximately 20 VMs each.


## <a id="sql-server"></a> SQL Server

The Internal MySQL database provided by PAS is sufficient for production use.


## <a id="blobstore-storage-accts"></a>  Blobstore storage accounts

Azure Blob Storage provides fully-redundant hot, cold, or archival storage in either local, regional, or global offerings. It is recommended to use Azure Blob Storage as the external File Storage to provide unlimited scaling and redundancy for high-availability deployments of PCF.

PAS needs buckets  created for:



*   BOSH Blobstore
*   Build Packs
*   Droplets
*   Packages
*   Resources

These bucket should have an associated role for read/write access.

## <a id="identity-management"></a> Identity management

Azure uses Managed Service Identities to handle service authorization against Azure. We recommend using unique Service Identities for deploying Operations Manager, Bosh Director, Microsoft Azure Service Broker, along with any other Tiles that require independent credentials. Each of these Service Identities should be scoped to the least privilege necessary to operate.
