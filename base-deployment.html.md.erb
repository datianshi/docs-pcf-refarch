---
breadcrumb: Pivotal Cloud Foundry Documentation
title: PCF Base Reference Architecture
owner: Customer0
---

## <a id="intro"></a>Introduction

A Pivotal Cloud Foundry (PCF) reference architecture describes a proven
approach for deploying Pivotal Cloud Foundry on a specific IaaS, such
as AWS, Azure, GCP, or vSphere, that meets the following requirements:

  - Secure
  - Publicly-accessible
  - Includes common PCF-managed services such as MySQL, RabbitMQ, and
    Spring Cloud Services
  - Can host at least 100 app instances, or far more

These documents detail PCF reference architectures for different IaaSes
to help you determine the best configuration for your PCF deployment.

Pivotal has validated the following PCF products on its own deployments
based on these reference architectures:

  - Pivotal Application Service (PAS)
  - Pivotal Cloud Foundry Ops Manager
  - Pivotal Container Services (PKS)

The PCF Base Reference Architecture is structured to cover common
reference architecture base components of PCF.

## <a id="pas"></a>PAS Topology

![PAS Base Deployment Topology](./images/v2/export/PAS_Base.png)

### <a id="pas-components"></a>Infrastructure Components

| **Component**           | **Notes**                                                                                                                                         |
|---------------------- |----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Ops Manager**         | Deployed on one of the three public subnets and accessible by FQDN or through an optional jumpbox.                                                                       |
| **BOSH Director**       | Deployed on the infrastructure subnet.                                                                                                                                   |
| **Jump Box**            | Optional. Deployed on the infrastructure subnet for accessing PAS management components such as Opsmanager and CF CLI access.                                            |
| **Gorouters**           | Accessed through the HTTP, HTTPS, and SSL Elastic Load Balancers. Deployed on all three Pivotal Application Service (PAS) subnets, one per AZ.                           |
| **Diego Brains**        | Required. However, the SSH container access functionality is optional and enabled through the SSH Elastic Load Balancers. Deployed on all three PAS subnets, one per AZ. |
| **TCP Routers**         | Optional feature for TCP routing. Deployed on all three PAS subnets, one per AZ.                                                                                         |
| **Service Tiles**       | Service brokers and shared services instances are deployed on Services subnet. Dedicated on-demand service instances are deployed on On-demand services subnet.          |
| **Isolation Segments**  | Isolation segments deployed on Isolation subnet includes Gorouter and Diego Cells for running application containers.                                                    |

### <a id="pas-network"></a>Network Considerations

#### <a id="pas-subnets"></a>Subnets Requirements

PAS requires a number of statically-defined networks to host the main
elements it’s composed of.

  - Infrastructure subnet - `/24` segment
  - PAS subnet - `/24` segment
  - Services subnet - `/24` segment
    The Services network is handy for use with Ops Manager tiles that
    you might add in addition to PAS. You can think of the Services
    network as the “everything else not PAS” network. Some of these
    tiles can call for additional network capacity to grow into on
    demand, so it is recommended that you consider adding an “On-Demand”
    network per tile that would need this and pair them up in the tile’s
    config.
  - On-demand Services subnet(s) - `/24` segments
    Example: Redis tile asks for “Network” and “Services Network”. The
    first one is for placing the broker and the second one is for
    deploying worker VMs to support the service. For this, we would
    deploy a new network “OD-Services1” and tell Redis to use the
    “Services” network for the broker and the “OD-Services1” network
    for the workers. The next tile can also use “Services” for the
    broker and a new “OD-Services2” network for workers, and so on.
  - Isolation Segments subnet(s) - `/24` segments
    Example: Isolation Segments can be added quickly and easily to an
    existing PAS installation. This range of address space is designated
    for use when adding one of more of them. A new /24 network for this
    range should be deployed for each new Isolation Segment. There’s
    reserved capacity for \>50 Isolation Segments of \>250 VMs per in
    this address bank.

#### <a id="pas-lb"></a>Load Balancing For PAS

In general, a suitable load balancer must be found to send incoming
HTTP, HTTPS,SSH and SSL traffic to the Gorouters and application
containers when applicable. All installations approaching Production
level use with make use of external load balancing from hardware
appliance vendors or other network-layer solutions.

The load balancer can also function as Layer 4 or Layer 7 load balancing
function. SSL can be terminated at the load balancer or used as a
pass-through to Gorouter.

Common deployments of load balancing in PAS are:

  - HTTP/HTTPS traffic to/from GoRouters
  - TCP traffic to/from TCP routers
  - Diego Brains for when application container SSH access is desired

Global Load Balancer for multiple PAS foundations can be achieved with
vendor specific Global Traffic Manager or Global DNS load balancer. For
additional Information :
[*https://docs.pivotal.io/pivotalcf/2-5/plan/global-dns-lb.html*](https://docs.pivotal.io/pivotalcf/2-5/plan/global-dns-lb.html)

### <a id="pas-compute"></a>Compute and HA Considerations

Considering PAS, a consolidation ratio for containers should follow a
conservative 4:1 ratio of vCPUs to pCPUs. An even more conservative,
meaning safe, consideration is 2:1. While this may seem “light” compared
to standard practice, keep in mind that standard modeling is based on
one OS and one (or only a few) apps per VM. With PAS, you will quickly
get to a dozen apps per VM.

A standard Diego cell by default is 4x16 (XL). At a 4:1 ratio you have a
16 vCPU budget, or about 12 containers after some waste running the OS.
If you want to get to more containers in a cell, be sure to scale the
vCPUs accordingly, with the caveat that high core count VMs become
increasingly hard to schedule on the pCPU without high physical core
counts in the socket.

#### <a id="pas-ha"></a>High Availability considerations

PAS isn’t considered HA until at least two Availability Zones are
defined (three are recommended). Using vSphere HA capabilities in
conjunction with PCF HA capabilities is the best of all worlds. PCF
gains redundancy thru the AZ construct and the loss of an AZ isn’t
considered catastrophic. Using Bosh Resurrector can replace lost VMs as
needed to repair a foundation. Foundation backup and restoration is
accomplished externally by [*BBR*](https://docs.cloudfoundry.org/bbr/).

### <a id="pas-storage"></a>Storage Considerations

**Capacity**

TBD: any generic notes on storage/blobstore? Or leave it to IaaS specific sections
only?  Internal and external.

### <a id="pas-security"></a>Security Considerations

#### <a id="pas-certs-tls"></a>Certificates and TLS

[Certificates and TLS in PCF](../security/pcf-infrastructure/certificates-index.html)

#### <a id="pas-firewall-ports"></a>Firewall Ports

[Network Communication Paths in PCF](../security/networking/#net_commpaths)

#### <a id="pas-roles-users"></a>IAM Roles and Users

### <a id="pas-domains"></a>Domain Names

PAS requires following domain names to be registered when creating a
wildcard certificate and also

PAS tile configurations

System domain: `sys.domain.name` - for PAS and accompanying tiles

App domain: `app.domain.name` - for your application domain.

Following wildcard domain names must be included when requesting
certificates in addition to app and system domains.

  - \*.SYSTEM-DOMAIN
  - \*.APPS-DOMAIN
  - \*.login.SYSTEM-DOMAIN
  - \*.uaa.SYSTEM-DOMAIN

### <a id="pas-scaling"></a>Scaling Considerations

TBD: any generic notes on scaling? Or leave it to IaaS specific sections
only? Link to docs?

## <a id="pks-topology"></a>PKS Topology

![PKS Base Deployment Topology](./images/v2/export/PKS_Base.png)

### <a id="pks-components"></a>Infrastructure Components

PKS is deployed as a tile by PCF Ops Manager and Bosh Director.

A PKS API server is deployed as a service broker VM on a PKS Services
subnet. The API server deploys and managers PKS clusters. It also
provides an API endpoint for the client to authenticate and manage PKS
cluster and a service adapter.- more information - see
[Pivotal Container Service (PKS)](https://docs.pivotal.io/runtimes/pks/index.html#components)

Table describing all PKS common components (e.g. VMs).

PKS Clusters are usually deployed to a dedicated subnet which includes
the worker VMs and one or more Master nodes per cluster.

Other service tiles, e.g. Harbor Docker Registry, are usually deployed
as part of the Services subnet.

### <a id="pks-network"></a>Network Considerations

#### <a id="pks-subnets"></a>Subnets Requirements

PKS requires two defined networks to host the main elements it’s
composed of.

  - Infrastructure subnet - `/24`
    It will host Ops Manager, Bosh Director, jump box for the control
    plane.
  - PKS Services subnet - `/24`
    It will host PKS API VM and other optional service tiles such as
    Harbor.
  - PKS Clusters subnets - each one a `/24` from a pool of IPs
    pre-allocated for PKS clusters

#### <a id="pks-lb"></a>Load Balancing For PKS

TBD

### <a id="pks-compute"></a>Compute and HA Considerations

TBD

#### <a id="pks-ha"></a>High Availability considerations

PKS has no inherent HA capabilities to design for. Make the best efforts
to have HA design at the IaaS, storage, power and access layers to
support PKS.

### <a id="pks-storage"></a>Storage considerations

PKS requires shared storage across all Availability Zones for the
deployed workloads to appropriately allocate their required storage.

**Capacity**

TBD: any generic notes on storage and capacity planning? Or leave it to
IaaS specific sections only?

#### <a id="pks-security"></a>Security Considerations

#### <a id="pks-certs-tls"></a>Certificates and TLS

[Certificates and TLS in PCF](../security/pcf-infrastructure/certificates-index.html)

#### <a id="pks-firewall-ports"></a>Firewall Ports

[Network Communication Paths in PCF](../security/networking/#net_commpaths)

#### <a id="pks-roles-users"></a>IAM Roles and Users

### <a id="pks-domains"></a>Domain Names

PKS requires the following domain names to be registered when creating a
wildcard certificate and also

PKS tile configurations: `\*.pks.domain.name`

So both the PKS API domain e.g. `api.pks.domain.name` and the PKS Clusters
domains e.g. `mycluster.pks.domain.name` are supported.

### <a id="pks-scaling"></a>Scaling Considerations

TBD: any generic notes on scaling? Or leave it to IaaS specific sections
only?

## <a id="iaas-specific">IaaS-specific Reference Architecture Topics

- [PCF on AWS](https://docs.google.com/document/d/1sPTEtUlBXsZ5qRlC4blCKQipmBFf1nz6TuXohDjR3gQ)
- [PCF on Azure](https://docs.google.com/document/d/1Zd4ptXxn-wlPNHbxTG07NALl84IJfL3B_hBfRT04qfg)
- PCF on vSphere (TBD)
- PCF on GCP (TBD)
